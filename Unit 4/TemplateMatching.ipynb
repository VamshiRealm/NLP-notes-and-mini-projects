{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963a8cc5-7eb3-4956-abf7-219037890324",
   "metadata": {},
   "source": [
    "<h1>Template Matching</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce038b9-959d-47f7-97b9-ee148c04b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Load the input image and the template image (heart)\n",
    "card_img = cv.imread(r\"C:\\Users\\vamsh\\Desktop\\NLP Images\\Card.png\")\n",
    "template = cv.imread(r\"C:\\Users\\vamsh\\Desktop\\NLP Images\\heart.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96c258d-fba8-4f5e-8edd-09d683abcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both images to grayscale\n",
    "gray_image = cv.cvtColor(card_img, cv.COLOR_BGR2GRAY)\n",
    "gray_template = cv.cvtColor(template, cv.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d046db8-4379-4431-95ec-1f1914cdc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a lower threshold for matching\n",
    "threshold = 0.6 #Lowered threshold for partial matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d036c88b-9365-4c8c-8896-3ab00a29c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform template matching at multiple scales\n",
    "matches_found = False\n",
    "all_locations = []\n",
    "\n",
    "# Get the dimensions of the original image\n",
    "img_height, img_width = gray_image.shape\n",
    "\n",
    "# Try matching template with different sizes or scales\n",
    "for scale in np.arange(0.5, 1.5, 0.1):   #Scale from 50% to 150%\n",
    "    resized_template = cv.resize(gray_template, (0,0), fx=scale, fy=scale)\n",
    "    w, h = resized_template.shape[::-1]\n",
    "\n",
    "    # Check if the template size is larger than the image size at the current size at the current\n",
    "    if w > img_width or h > img_height:\n",
    "        continue  #Skips the current iteration and moves to the next scale\n",
    "\n",
    "    result = cv.matchTemplate(gray_image, resized_template, cv.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # Find locations where the match is greater than the threshold\n",
    "    locations = np.where(result >= threshold)\n",
    "\n",
    "    # This loop extracts and stores the detected template match locations of template height and width\n",
    "    for pt in zip(*locations[::-1]): #Reverse the order of locations\n",
    "        all_locations.append((pt[0], pt[1], w, h))\n",
    "\n",
    "# all_locations - stores all the detected template positions\n",
    "if len(all_locations) > 0:\n",
    "    boxes = np.array(all_locations)\n",
    "    x_coords = boxes[:, 0] #Extract top-left x-coordinate of matched template\n",
    "    y_coords = boxes[:, 1] #Extract top-left y-coordinates\n",
    "    widths = boxes[:, 2] #Width of each template\n",
    "    heights = boxes[:, 3] #Height of each template\n",
    "\n",
    "    # Compute the bottom-right x and y coordinate\n",
    "    x2_coords = x_coords + widths\n",
    "    y2_coords = y_coords + heights\n",
    "\n",
    "    # This combines all four arrays (x1, y1, x2, y2) into a single NumPy array and transpose to draw bounding box\n",
    "    boxes = np.array([x_coords, y_coords, x2_coords, y2_coords]).T\n",
    "\n",
    "    # Perform NMS (non Max supression) using OpenCV function\n",
    "    # NMS- helps to remove unnecessary overlapping boxes\n",
    "    # [1]*len(boxes) - compute configure score, 1-dummy score, len(boxes) - no.\n",
    "    indices = cv.dnn.NMSBoxes(boxes.tolist(), [1]*len(boxes), score_threshold=0.6, nms_threshold=0.3)\n",
    "\n",
    "    # Draw the bounding boxes\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x1, y1, x2, y2 = boxes[i]\n",
    "            cv.rectangle(card_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        matches_found = True\n",
    "    \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c52b67-fc87-4e44-9715-c51b0deba1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result with bounding boxes\n",
    "cv.imshow('Detected Hearts on Original Image', card_img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d02592-6a48-406e-b99f-18c0c0868b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
