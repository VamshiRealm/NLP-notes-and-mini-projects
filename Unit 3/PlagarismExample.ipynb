{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1925ac29-ef24-460c-b45e-40809bf8ca8f",
   "metadata": {},
   "source": [
    "<h1>Plagarism Checker</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b48441ef-1689-4b2d-9022-ee83f7d7ef76",
   "metadata": {},
   "source": [
    "Types of plagarasm:\n",
    "1. Direct Plagarism\n",
    "2. Self Plagarism\n",
    "3. Mosiac Plagarism\n",
    "4. Rephrasing Plagarism\n",
    "5. AI Plagarism\n",
    "\n",
    "Tools for checking plagarism:\n",
    "-Ternitter\n",
    "-Grammarly\n",
    "-Plagcheck\n",
    "\n",
    "\n",
    "We use 2 docs to understand:\n",
    "Local :  ours\n",
    "other: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295aa81b-6c08-473a-a2bf-46143f155667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8d7f3-881b-4511-b87e-c4344bb6640b",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30490855-26ce-4302-960c-751a6e714abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(docs):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Apply preprocessing to each doc individually\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(doc)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words ]\n",
    "        processed_docs.append(\" \".join(tokens))\n",
    "        \n",
    "    return processed_docs\n",
    "\n",
    "#str.maketrans(from_chars, to_chars, delete_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e6cc42-3242-448c-a436-5b3eac9d8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a translation table to replace 'a' with 'x' and 'b' with 'y' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006d349e-673f-4015-ab69-42fa73f4d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love programming python\n",
      "python great language programming\n",
      "enjoy coding learning new language\n"
     ]
    }
   ],
   "source": [
    "# List of docs \n",
    "docs = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is a great language for programming.\",\n",
    "    \"I enjoy coding and learning new languages.\"\n",
    "]\n",
    "\n",
    "# Preprocess the each of docs individually \n",
    "processed_text = preprocess_text(docs)\n",
    "\n",
    "# Print the processed text\n",
    "for text in processed_text:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e14f6-796d-46fc-a164-5f932526d7d7",
   "metadata": {},
   "source": [
    "# Function to calcuate Similarity between text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea32a67e-f3ea-4f3c-9b8f-bc478b935140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.47627592 0.        ]\n",
      " [0.47627592 1.         0.16344687]\n",
      " [0.         0.16344687 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(docs):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # the resultin Tfidf_matrix is a sparse matrix of shape(no. of docs * no. of docs)\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "    return cosine_similarity(tfidf_matrix)\n",
    "\n",
    "similarity_matrix = calculate_similarity(processed_text)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa195d9-3c8a-4be5-a037-c13dd1a38c85",
   "metadata": {},
   "source": [
    "# Plagarism Checker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58f0afa9-1f9d-471c-a1d6-e6152abc73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local - doc you want to check\n",
    "\n",
    "def check_plagarism(local_docs, sample_docs, threshold=0.3):\n",
    "    # Combine all docs & compare together in one list\n",
    "    all_docs = local_docs + sample_docs\n",
    "\n",
    "    # List Comprehension - Call preprocess_text function\n",
    "    preprocessed_docs = [' '.join(preprocess_text(doc)) for doc in all_docs]\n",
    "\n",
    "    # Call calculate_similarity fun \n",
    "    similarity_matrix = calculate_similarity(preprocessed_docs)\n",
    "    print(similarity_matrix)\n",
    "    # Output Plagarism Results\n",
    "    print(\"Plagarism results: \")\n",
    "    for i in range(len(local_docs)):\n",
    "        for j in range(len(local_docs), len(all_docs)):\n",
    "            similarity_score = similarity_matrix[i][j]\n",
    "            if similarity_score > threshold:\n",
    "                print(f\"Local Document {i+1} is plagarized with Sample doc {j- len(local_docs)+1} with Similarity score: {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ee97f-8da8-43b5-af0d-c599301a406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4e421c5-ffc1-41d4-9102-d68993d9c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed Sample Documnets: \n",
      "['', 'h', 'e', '', 'b', 'e', '', '', '', 'w', '', '', '', '', '', '', '', 'c', 'h', '', 'e', 'v', 'e', '', 'g', 'r', 'e', '', '', '', '', 'h', '', 'n', 'n', 'g', '', '', '', '', '', '', '', '', 'l', '', 'v', 'e', '', 'w', 'h', '', '', '', '', '', 'u', '', '', '', '']\n",
      "['h', '', 'p', 'p', '', 'n', 'e', '', '', '', '', '', '', '', 'h', 'e', '', '', 'r', 'u', 'e', '', 'k', 'e', '', '', '', '', '', '', 'u', 'c', 'c', 'e', '', '', '', '', 'n', '', '', '', '', 'h', 'e', '', '', '', 'h', 'e', 'r', '', 'w', '', '', '', '', 'r', '', 'u', 'n', '', '']\n",
      "['e', 'v', 'e', 'r', '', '', '', '', 'f', 'f', '', 'c', 'u', 'l', '', '', '', 'h', '', 'l', '', '', '', '', 'n', '', '', 'p', 'p', '', 'r', '', 'u', 'n', '', '', '', '', 'w', '', '', 'h', '', 'n', '']\n",
      "\n",
      "Checking for plagarism...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# step 2: ckeck plagarism\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChecking for plagarism...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mcheck_plagarism\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mcheck_plagarism\u001b[1;34m(local_docs, sample_docs, threshold)\u001b[0m\n\u001b[0;32m      8\u001b[0m preprocessed_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(preprocess_text(doc)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_docs]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Call calculate_similarity fun \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(similarity_matrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Output Plagarism Results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# the resultin Tfidf_matrix is a sparse matrix of shape(no. of docs * no. of docs)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity(tfidf_matrix)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Main Workflow\n",
    "if __name__ == \"__main__\":\n",
    "    local_docs = [\n",
    "        \"The only way to do great work is to love what you do.\",\n",
    "        \"Success is not the key to happiness. Happiness is the key to success.\",\n",
    "        \"In the middle of every difficulty lies opportunity.\"\n",
    "    ]\n",
    "\n",
    "    sample_docs = [\n",
    "        \"The best way to achieve great thinngs is to love what you do.\",\n",
    "        \"Happiness is the true key to success, not the other way around.\",\n",
    "        \"Every difficulty holds an opportunity within.\"\n",
    "    ]\n",
    "\n",
    "    # step1: preprocess the sample document\n",
    "    print(\"\\nPreprocessed Sample Documnets: \")\n",
    "    # for i, doc in enumerate(sample_docs, 1):\n",
    "    #     print(f\"Document {i}: {preprocess_text(doc)}\")\n",
    "    for doc in sample_docs:\n",
    "        print(preprocess_text(doc))\n",
    "\n",
    "    # step 2: ckeck plagarism\n",
    "    print(\"\\nChecking for plagarism...\")\n",
    "    check_plagarism(local_docs, sample_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8bceb-186e-47d4-9a25-e04d3545a801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
